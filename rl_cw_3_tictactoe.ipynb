{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87c81e84938be381d494b523e04a0fc3",
     "grade": false,
     "grade_id": "cell-7f67b219ed6b3541",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 3: Tic-Tac-Toe\n",
    "\n",
    "In this piece of coursework, you will implement the game of Tic-Tac-Toe and a Q-Learning agent capable of playing it.\n",
    "\n",
    "**Total number of marks:** 40 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include all of your source code. Please do not change the file name or compress/zip your submission. Please do not include any identifying information on the files you submit. This coursework will be **marked anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and write your own code.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **3 minutes** on the University's computers in 10W 0.02 and that **written answer length limits** are adhered to. Otherwise, you may not get credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tic-Tac-Toe Environment\n",
    "For this exercise, you should implement the game of [Tic-Tac-Toe](https://en.wikipedia.org/wiki/Tic-tac-toe) (also known as Noughts and Crosses). After you have implemented the environment, you will validate it by letting two random agents play against each other.\n",
    "\n",
    "### The Tic-Tac-Toe Environment\n",
    "\n",
    "Tic-Tac-Toe is a simple game for two players, O and X, who take turns marking cells in a 3x3 grid. The player who succeeds in drawing three instances of their symbol in a horizontal, vertical or diagonal line wins the game. The following example, from [Wikipedia](https://commons.wikimedia.org/wiki/File:Tic-tac-toe-game-1.svg), shows a game that is won by the X player.\n",
    "\n",
    "<img src=\"images/tic-tac-toe_example.svg\" style=\"width: 600px;\"/> \n",
    "\n",
    "### Instructions\n",
    "Implement the game of Tic-Tac-Toe. The first-moving player should be randomly selected at the beginning of each episode. Player X should always be a random agent (i.e. it places an X symbol in a random empty grid-space each time-step). You will implement various agents for player O. Rewards of +1, -1 and 0 should be collected for winning, losing and drawing respectively. All other rewards collected during the game should be 0.\n",
    "\n",
    "Test your Tic-Tac-Toe implementation by letting two random agents play against each other for 100 episodes. Plot the **cumulative rewards** collected by both the O and X agents as a function of the number of episodes.\n",
    "\n",
    "Hint: The X player can be considered to be part of the environment, as it is outside the agent's control.\n",
    "\n",
    "**Exercises 1, 2, and 4 will be marked jointly; you will see this joint score in Exercise 4. You will be given marks for the strength of your implementation and your understanding of the techniques you have implemented. Please note that the standard implementation of Q-learning will receive at most 8 marks from exercise 3 and 0 marks from exercises 1, 2, and 4. This is the most challenging part of your coursework. Precise instructions are given below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5fba896ce5c0ace5ce0cfcf0253fd0d",
     "grade": true,
     "grade_id": "cw3_tictactoe_environment",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your Tic-Tac-Toe Environment, your random O agent, and produce your cumulative reward plot.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Q-Learning Agent\n",
    "In this exercise, you will implement an agent which learns an optimal policy for playing Tic-Tac-Toe against a random opponent using Q-Learning. For your reference, the pseudocode for the Q-Learning algorithm is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131).\n",
    "\n",
    "<img src=\"images/q_learning_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "In order to score high marks in this coursework, you will need to extend your solution beyond a simple Q-Learning agent to achieve more efficient learning (i.e., using fewer interactions with the environment). Ideas for improving your agent will have been discussed in lectures, and can be found in the course textbook (Reinforcement Learning, Sutton & Barto, 2018). However you go about improving your agent, it must still use tabular Q-Learning at its core. \n",
    "\n",
    "Please produce the following:\n",
    "- A Q-Learning agent for the O player, with whatever modifications you believe are reasonable in order to acheieve good performance in the game of Tic-Tac-Toe.\n",
    "- A learning curve for your agent. This learning curve should plot the performance of a single agent, and should be smoothed by taking the mean of the cumulative rewards collected by the agent every 20 episodes. You should not smooth your learning curve in any other way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea774c3b2f522c7578217aa5ac60325c",
     "grade": true,
     "grade_id": "cw3_tictactoe_agent",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your Q-Learning agent and produce your average learning curve plot.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Optimal Policy (8 Marks)\n",
    "In this exercise, we will test your agent's policy to see whether it has learned an optimal policy.\n",
    "\n",
    "Please define a function `test_policy(state)` below, which takes a state and returns the action that your agent would take in that state. **This function should not re-train an agent every time it is called**, it should use the policy that was learned by the agent that you trained in the previous exercise.\n",
    "\n",
    "We will represent the state as a list of nine characters, either `'X'`, `'O'` or `''` for cells occupied by X, O, an neither player respectively. The list index corresponding to each of the nine grid-cells is as follows:\n",
    "\n",
    "<img src=\"images/tic_tac_toe_grid_indexes.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "The integer your `test_policy(state)` function returns should be the index corresponding to the cell your agent would place its O symbol in.\n",
    "\n",
    "We will be testing a number of states. Among them is the following state:\n",
    "\n",
    "<img src=\"images/tic_tac_toe_question_state.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "This state will be passed into your `test_policy(state)` function as the list `['O', 'X', '', '', 'X', '', '', 'O', '']`. If, for example, your agent's policy is to place its symbol in the top-right cell, your function should return the integer value `2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e4f553db30f83b89cfe9b04658f2a28",
     "grade": false,
     "grade_id": "cw3_tic_tac_toe_optimal_policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your test_policy(state) function here.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "def test_policy(state) :\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f512db2ab299bdc1a1e19677a66b5f5",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6894e95017470dcf3c37e2c7d1c1692",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9604e4ce8475bf9837276485ba9fffb5",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e49ed4aa7054c0f57b60bf0a74474faa",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13e65d6473a729d1937d83d45a469d74",
     "grade": false,
     "grade_id": "cell-5003798812b53f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Discussion (32 Marks)\n",
    "\n",
    "In eight sentences or fewer, please discuss the following:\n",
    "- Your chosen state representation.\n",
    "- Any additions that you have made to your implementation beyond implementing basic Q-Learning.\n",
    "- The effects you expected your additions to have, and the extent to which your expectations were met.\n",
    "- Further modifications you believe may enhance the performance of your agent, or changes you would make if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb2d64b7664c050da9cb9d13ff54d42b",
     "grade": true,
     "grade_id": "cell-cw3_tic_tac_toe_discussion",
     "locked": false,
     "points": 32,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Please write your answer in this markdown cell.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
